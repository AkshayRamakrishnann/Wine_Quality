# -*- coding: utf-8 -*-
"""Wine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wD_XKsUaEsyZr4xaDtzrf9DlFaJCIxWi
"""

pip install scikit-learn-intelex

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import scipy.stats as st
import plotly.graph_objects as go
import missingno as msno
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
from sklearn import preprocessing
from sklearn.metrics import f1_score, confusion_matrix,accuracy_score, classification_report
from scipy import stats

from google.colab import files
files.upload()

df= pd.read_csv('WineQT.csv')
df.head()

df.isnull().sum()

from imblearn.over_sampling import SMOTE

oversample = SMOTE()
features, labels=  oversample.fit_resample(df.drop(["quality"],axis=1),df["quality"])

scaler = preprocessing.MinMaxScaler()
names = features.columns
d = scaler.fit_transform(features)

scaled_df = pd.DataFrame(d, columns=names)
scaled_df.head()

X_train, X_test, y_train, y_test=train_test_split(scaled_df,labels,test_size=0.33,random_state=42)

y_train_bin = np_utils.to_categorical(y_train)
y_test_bin = np_utils.to_categorical(y_test)

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from keras.layers import Dropout
from keras import optimizers

info_models = []
model = Sequential()
model.add(Dense(X_train.shape[1]-3, input_dim=X_train.shape[1], activation='relu'))
# model.add(Dropout(.9))
# model.add(Dense(6, activation='relu'))
model.add(Dense(5, activation='relu' ))
# model.add(Dense(15, activation='relu' ))
# model.add(Dropout(.4))
# model.add(Dropout(.5))
model.add(Dense(9, activation='softmax'))
  # Compiling model
model.compile(loss='categorical_crossentropy', optimizer='adam', 
                      metrics=['accuracy'])
model2 = model
j_model = model.to_json()

for _ in range(1):
    model = keras.models.model_from_json(j_model)
    model.compile(loss='categorical_crossentropy', optimizer='adam', 
                      metrics=['accuracy'])
        # Training a model
    model.fit(X_train, y_train_bin, epochs=200, batch_size=10,verbose=0);
    scores = model.evaluate(X_test, y_test_bin)
    score = scores[1]*100
    info_model = {'score':score,'weights':model.get_weights()}
    info_models.append(info_model.copy())
    print("\nAccuracy: %.2f%%" % (scores[1]*100))
scores = [score.get('score') for score in info_models]
print(max(scores))

scores = [score.get('score') for score in info_models]
print(scores.index(max(scores)))
max_index = scores.index(max(scores))
weights = info_models[max_index].get('weights')

model2.set_weights(weights)

scores = model2.evaluate(X_test, y_test_bin)
print("\nAccuracy: %.2f%%" % (scores[1]*100))

import torch
import torch.nn as nn
import torch.optim as optim

import numpy as np
import torch.nn.functional as F

from sklearn.model_selection import train_test_split
from torch.autograd import Variable
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.preprocessing import OneHotEncoder, scale

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
import os

random.seed(1)
torch.manual_seed(1)
torch.cuda.manual_seed(1)
np.random.seed(1)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# define the test percentage = 20%
test_per = 0.2

# number of input features
n_features = 11

class FCN(nn.Module):
    def __init__(self):
        super(FCN , self).__init__()
        
        # Input to 11 features (n_features defined as 11 above) to 11 hidden nodes. 
        self.layer1 = nn.Linear(n_features , n_features)
        
        # 11 hidden nodes to 6 output classes.
        self.layer2 = nn.Linear(n_features , 6)  # 6 output classes
        
        # Forward pass
    def forward(self , data):
        
        # apply layer one to input 
        activation1 = self.layer1(data)
        
        # sigmoid activation on the first layer
        activation1 = torch.sigmoid(activation1)
        
        # layer two 
        activation2 = self.layer2(activation1)
        
        # return the output activation on the sigmoid
        return torch.sigmoid(activation2)

oneHot = OneHotEncoder()

# Fit OneHotEncoder to X, then transform X. Then to an array
label = oneHot.fit_transform(df).toarray()

model = LGBMClassifier()
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)
print(accuracy)

from sklearn.ensemble import RandomForestClassifier  
classifier= RandomForestClassifier(n_estimators= 10, criterion="entropy")  
classifier.fit(X_train, y_train)  
accuracy = classifier.score(X_test, y_test)
print(accuracy)

